{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN CODE FILE with EXPLAINATION**\n",
        "\n",
        "1.   Step 1: Import Required Libraries\n",
        "2.   Step 2: Define Function to Extract Q&A Pairs from PDFs\n",
        "3. Step 3: Load PDFs and Create Knowledge Base\n",
        "4. Step 4: Define the Function to Find Answers in the Knowledge Base\n",
        "5. Step 5: Chatbot Response and Running the Chatbot\n"
      ],
      "metadata": {
        "id": "o5y8c-6om-MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I have used Colab to perform this assignment, following are the dependencies I have installed."
      ],
      "metadata": {
        "id": "fTl-G7ajnOLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fUKLKqETjV_3",
        "outputId": "ae692c92-6cb6-4eba-92f3-a5dc9c7c0c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 python-Levenshtein-0.25.1 rapidfuzz-3.9.7\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.46.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_core-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading openai-1.46.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.0/375.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.1-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.123-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jiter, h11, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.1 langchain-text-splitters-0.3.0 langsmith-0.1.123 openai-1.46.0 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-Levenshtein\n",
        "!pip install fuzzywuzzy\n",
        "!pip install openai langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the final code to run PDF based chatbot."
      ],
      "metadata": {
        "id": "HcwOrB0InX_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Conversation history to maintain dialogue\n",
        "conversation_history = []\n",
        "\n",
        "# Function to extract Q&A pairs from PDFs\n",
        "def extract_qa_from_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    # Split text into Q&A pairs based on numbering (e.g., \"4. \", \"5. \")\n",
        "    qa_pairs = re.split(r'(\\d+\\.\\s+)', text)\n",
        "\n",
        "    # Merge the split text into a list of questions and answers\n",
        "    qa_list = []\n",
        "    for i in range(1, len(qa_pairs), 2):\n",
        "        question = qa_pairs[i].strip() + \" \" + qa_pairs[i + 1].strip()\n",
        "        qa_list.append(question)\n",
        "\n",
        "    # Create a dictionary of question-answer pairs\n",
        "    qa_dict = {}\n",
        "    for qa in qa_list:\n",
        "        if '\\n' in qa:\n",
        "            parts = qa.split('\\n', 1)\n",
        "        else:\n",
        "            parts = qa.split('.', 1)\n",
        "\n",
        "        if len(parts) > 1:\n",
        "            question = parts[0].strip()\n",
        "            answer = parts[1].strip()\n",
        "            qa_dict[question] = answer\n",
        "\n",
        "    return qa_dict\n",
        "\n",
        "# Define function to find answers in the knowledge base with fuzzy matching\n",
        "def find_answer_in_docs(query, knowledge_base, threshold=70):\n",
        "    query = query.lower()\n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "    best_answer = None\n",
        "    source = None\n",
        "\n",
        "    # Iterate over documents in the knowledge base\n",
        "    for doc_name, qa_dict in knowledge_base.items():\n",
        "        for question, answer in qa_dict.items():\n",
        "            score = fuzz.partial_ratio(query, question.lower())\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = question\n",
        "                best_answer = answer\n",
        "                source = doc_name\n",
        "\n",
        "    if best_score >= threshold:\n",
        "        return f\"Found in {source}:\\nQ: {best_match}\\nA: {best_answer}\\nSource URL: {source}\", best_match\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Define the function to return chatbot response\n",
        "def chatbot_response(query):\n",
        "    global conversation_history\n",
        "    answer, matched_question = find_answer_in_docs(query, knowledge_base)\n",
        "\n",
        "    if answer:\n",
        "        conversation_history.append(f\"User: {query}\")\n",
        "        conversation_history.append(f\"Chatbot: {answer}\")\n",
        "        return f\"{answer}\\n\"\n",
        "\n",
        "    # Suggest similar questions from the knowledge base based on keywords\n",
        "    suggestions = suggest_questions(query, knowledge_base)\n",
        "    if suggestions:\n",
        "        response = \"I'm sorry, I couldn't find an exact match. Did you mean one of the following?\\n\"\n",
        "        for suggestion in suggestions:\n",
        "            response += f\"- {suggestion}\\n\"\n",
        "        return response\n",
        "    else:\n",
        "        return \"I'm sorry, I couldn't find an answer to your question in the knowledge base.\\n\"\n",
        "\n",
        "# Function to suggest questions based on fuzzy matching\n",
        "def suggest_questions(query, knowledge_base, threshold=50):\n",
        "    query = query.lower()\n",
        "    suggestions = []\n",
        "\n",
        "    # Iterate over documents to find similar questions\n",
        "    for doc_name, qa_dict in knowledge_base.items():\n",
        "        for question in qa_dict.keys():\n",
        "            score = fuzz.partial_ratio(query, question.lower())\n",
        "            if score >= threshold:\n",
        "                suggestions.append(question)\n",
        "\n",
        "    return suggestions if suggestions else None\n",
        "\n",
        "# Function to run the chatbot interactively\n",
        "def run_chatbot():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        response = chatbot_response(query)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "# Example PDF paths (replace with actual paths)\n",
        "pdf_path_1 = '/content/FINAL_FAQs_June_2018.pdf'\n",
        "pdf_path_2 = '/content/Final_FREQUENTLY_ASKED_QUESTIONS_-PATENT.pdf'\n",
        "\n",
        "# Extract Q&A pairs from the PDFs\n",
        "pdf_text_1 = extract_qa_from_pdf(pdf_path_1)\n",
        "pdf_text_2 = extract_qa_from_pdf(pdf_path_2)\n",
        "\n",
        "# Combine the Q&A pairs into the knowledge base\n",
        "knowledge_base = {\n",
        "    'Document 1': pdf_text_1,\n",
        "    'Document 2': pdf_text_2\n",
        "}\n",
        "\n",
        "# Run the chatbot\n",
        "run_chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7MGaAmLHjvXJ",
        "outputId": "c32d05d7-7064-472f-f82f-ffa23d80b776"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: what is patent?\n",
            "Bot: Found in Document 2:\n",
            "Q: 1. What  is a Patent?\n",
            "A: A Patent is a stat utory right for an invention granted for a lim ited period of time to the patentee by \n",
            "the Government, in exchange of full disclosure of his invention for excluding others,  from making , \n",
            "using, selling , importing the patented product or process for producing that product for those \n",
            "purposes with out his consent.\n",
            "Source URL: Document 2\n",
            "\n",
            "You: Types of application\n",
            "Bot: Found in Document 2:\n",
            "Q: 23. What a re the types of applicatio ns?\n",
            "A: The types of applications that can be filed ar e: \n",
            "A) PROVIS IONAL APPLICATION \n",
            "Indian Patent Law follows first to file system. A provisional applicatio n is an applicatio n which  can be \n",
            "filed if the invention is still under experimentati on stage. Filing a provisional specificat ion provides \n",
            "the  advantage  to  the  inventor  since  it  helps  in establishing a  ―priority  date  of  the  invention. \n",
            "Further,  the inventor gets 12 months’ time to fully develop the invention and ascertain  its market \n",
            "potential and to file the complete specificat ion. \n",
            " \n",
            "B) ORDINARY APPLICATION  \n",
            " \n",
            "An  applicatio n  for  patent  filed  in  the  Patent  Office  without  claiming  any  priority  either  in  a \n",
            "convention country or wit hout any reference  to any other earli er applicat ion under process in the \n",
            "office. Such type of application is known an  ordinary applicatio n. \n",
            " \n",
            "C) CONVENTION  APPLICATION  \n",
            " \n",
            "An applicatio n for patent filed in the Patent Office, claiming a priority date based on the same or \n",
            "substantial ly similar applicatio n filed in one or more of the convention countries  is known as a \n",
            "convention application. In order to get convention status,  an a pplican t should f ile the applicatio n in \n",
            "the Indian Patent Offic e within 12 months from the date of first filing of a sim ilar applicatio n in the \n",
            "convention country. \n",
            " \n",
            "D) PCT INTERNATIO NAL APPLICATION \n",
            " \n",
            "An A pplicatio n filed in India as Receiving Office  (RO) under Patent Cooperation Treaty is an \n",
            "international applicatio n which can be filed in more than 150 countries  by a single ap plicat ion. Office of CGP DTM, INDIA |  www.ipindia.go v.in Page 7 \n",
            "Frequent ly Asked Questi ons 2020  \n",
            "  \n",
            "E) PCT NATIO NAL PHASE APPLICATION  \n",
            " \n",
            "When an i nternational ap plicatio n is made according to PCT designating India, an a pplican t can fil e \n",
            "the national phase applicatio n in India within 31 months from the international filing date or the \n",
            "priority  date,  whichever is earlier.  \n",
            " \n",
            "F) PATENT OF ADDITION  \n",
            " \n",
            "When  an invention is a slight modification of the earlier invention for which  he has already  applied \n",
            "for or has obtained patent, the applicant can go for patent of add ition if the mod ification in the \n",
            "invention is new. One of the benefits  of filing patent of addition is that there no need to pay \n",
            "separ ate renewal fee for the patent of addition during the term of the main patent and it expires \n",
            "along with the main patent. \n",
            " \n",
            "G) DIVISIONAL APPLICATION \n",
            " \n",
            "When  an applicatio n claims more than  one invention, the applican t on his own or to meet the \n",
            "official objection on the ground of plurality or distin ct invention may divide  the applicatio n and file \n",
            "two or more applications, as  the case may be for each of the inventions. This type of applicat ion, \n",
            "divided  out  of  the  parent  one,  is  known  a  Divisional  Applicatio n.  The  priority  date  for  all  the \n",
            "divisio nal applications will be same as that of the main (the Parent) Application (Ante-dating).\n",
            "Source URL: Document 2\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bcdb8e2c50c6>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Run the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-bcdb8e2c50c6>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mconversation_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code explaination **\n",
        "\n",
        "**[link text](https://)Step 1: Import Required Libraries**\n",
        "os: Provides a way to interact with the operating system, although not directly used in the current code. It can be used for file management (e.g., checking file paths).\n",
        "re: This is Python’s regular expression library, used here to handle text matching and splitting operations.\n",
        "PyPDF2: A library used to read and extract text from PDF documents.\n",
        "fuzzywuzzy: A library used for string matching, which compares two strings and provides a similarity score.\n",
        "\n",
        "**Step 2: Define Function to Extract Q&A Pairs from PDFs**\n",
        "1. extract_qa_from_pdf: This function opens a PDF file and reads its content.\n",
        "2. file_path: The path to the PDF file that will be passed to this function.\n",
        "3. open(file_path, \"rb\"): Opens the PDF file in binary mode.\n",
        "4. PyPDF2.PdfReader(file): Creates a PdfReader object to read the contents of the PDF.\n",
        "for page_num in range(len(pdf_reader.pages)): Loops through each page in the PDF.\n",
        "5. text += page.extract_text(): Appends the extracted text from each page to the text variable.\n",
        "6. re.split(r'(\\d+.\\s+)', text): Splits the text into chunks based on the pattern (\\d+\\.\\s+), which matches numbers followed by a period and spaces (e.g., \"4. \", \"5. \").\n",
        "This helps break down the content into potential Q&A pairs.\n",
        "\n",
        "7. qa_list = []: Initializes an empty list to hold the extracted question-answer pairs.\n",
        "8. for i in range(1, len(qa_pairs), 2): Loops through the split text, processing every second element as a potential question, followed by its answer.\n",
        "9. question = qa_pairs[i].strip() + \" \" + qa_pairs[i + 1].strip(): Combines the question and answer, stripping any unnecessary whitespace.\n",
        "\n",
        "10. qa_dict = {}: Initializes an empty dictionary to store the final question-answer pairs.\n",
        "11. for qa in qa_list: Loops through the extracted questions and answers in qa_list.\n",
        "12. if '\\n' in qa: Checks if there is a line break (\\n) in the question-answer pair.\n",
        "13. parts = qa.split('\\n', 1): Splits the text at the first line break.\n",
        "14. parts = qa.split('.', 1): If no line break is found, it splits the text at the first full stop.\n",
        "15. if len(parts) > 1: Ensures that both the question and the answer are present after splitting.\n",
        "16.question = parts[0].strip(): Takes the first part as the question and removes any surrounding spaces.\n",
        "17. answer = parts[1].strip(): Takes the second part as the answer and removes any surrounding spaces.\n",
        "18. qa_dict[question] = answer: Stores the question and its corresponding answer in the qa_dict.\n",
        "19. return qa_dict: Returns the dictionary of question-answer pairs.\n",
        "\n",
        "**Step 3: Load PDFs and Create Knowledge Base**\n",
        "1. pdf_path_1, pdf_path_2: Paths to the PDF files.\n",
        "2. extract_qa_from_pdf(pdf_path_1): Calls the function to extract Q&A pairs from the first PDF.\n",
        "3. knowledge_base: Combines the extracted Q&A pairs from both PDFs into a dictionary. Each document is represented as a key, with its extracted content as the value.\n",
        "\n",
        "**Step 4: Define the Function to Find Answers in the Knowledge Base**\n",
        "1. find_answer_in_docs: Function that searches for the best matching answer from the knowledge base based on the user query.\n",
        "2. query.lower(): Converts the user query to lowercase for case-insensitive matching.\n",
        "3. best_match, best_score, best_answer: Variables to store the closest match, its similarity score, and the corresponding answer.\n",
        "4. for doc_name, qa_dict in knowledge_base.items(): Loops through the documents and their Q&A dictionaries.\n",
        "5. score = fuzz.partial_ratio(query, question.lower()): Uses fuzzy matching to compare the user query with each question in the knowledge base.\n",
        "6. if score > best_score: Updates the best_score and stores the matching question and answer if the current match has the highest score.\n",
        "7. if best_score >= threshold: If the similarity score is above the threshold (70 by default), it returns the matched question, answer, and the document source.\n",
        "8. else: If no match is found, the chatbot returns a fallback message saying it couldn't find an answer.\n",
        "\n",
        "**Step 5: Chatbot Response and Running the Chatbot**\n",
        "\n",
        "1. chatbot_response: This function processes the user query and retrieves the response from the knowledge base.\n",
        "2. find_answer_in_docs(query, knowledge_base): Calls the previously defined function to find an answer to the user's query.\n",
        "3. return f\"{answer}\\n\": Returns the answer, if found, or a default message.\n",
        "4. run_chatbot: Starts the chatbot interaction in a loop.\n",
        "query = input(\"You: \"): Accepts user input as a query.\n",
        "5. if query.lower() == \"exit\": Ends the loop if the user types \"exit\".\n",
        "response = chatbot_response(query): Calls the chatbot to get a response based on the user query.\n",
        "6. print(f\"Bot: {response}\"): Outputs the chatbot's response."
      ],
      "metadata": {
        "id": "b1p9o4rcoAM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oS5KW-TtplMO"
      }
    }
  ]
}